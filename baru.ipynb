{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "aca95325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from category_encoders import TargetEncoder, CountEncoder\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "0547e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('training_dataset.csv')\n",
    "test_df = pd.read_csv('validation_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "784db152",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(\n",
    "    train_df, test_size = 0.2,\n",
    "    random_state = 43,\n",
    "    stratify = train_df['berlangganan_deposito'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "792d0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "\n",
    "encoder1 = CountEncoder()\n",
    "encoder2 = CountEncoder()\n",
    "encoder_kerja = CountEncoder()\n",
    "\n",
    "def apply_count_encoding(df: pd.DataFrame, train: bool = True) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    if train:\n",
    "        ret['jenis_kontak'] = encoder1.fit_transform(ret['jenis_kontak'])\n",
    "        ret['kelompok_usia'] = encoder2.fit_transform(ret['kelompok_usia'])\n",
    "        ret['pekerjaan'] = encoder_kerja.fit_transform(ret['pekerjaan'])\n",
    "        # ret['hari_kontak_terakhir'] = encoder3.fit_transform(ret['hari_kontak_terakhir'])\n",
    "    else:\n",
    "        ret['jenis_kontak'] = encoder1.transform(ret['jenis_kontak'])\n",
    "        ret['kelompok_usia'] = encoder2.transform(ret['kelompok_usia'])\n",
    "        ret['pekerjaan'] = encoder_kerja.transform(ret['pekerjaan'])\n",
    "     \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "111e91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder3 = LabelEncoder()\n",
    "def apply_label_encoding(df: pd.DataFrame, train: bool = True) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    if train:\n",
    "        ret['hari_kontak_terakhir'] = encoder3.fit_transform(ret['hari_kontak_terakhir'])\n",
    "    else:\n",
    "        ret['hari_kontak_terakhir'] = encoder3.transform(ret['hari_kontak_terakhir'])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "67eea05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "def apply_scale(df: pd.DataFrame, train:bool = False) -> pd.DataFrame:\n",
    "  cols = [col for col in df.columns if col != \"berlangganan_deposito\" and df[col].dtype in [np.float64, np.int64]]\n",
    "  ret = df.copy()\n",
    "  if train:\n",
    "    ret[cols] = scaler.fit_transform(df[cols])\n",
    "  else:\n",
    "    ret[cols] = scaler.transform(df[cols])\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "9957ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    # ret['jumlah_pekerja_x_tingkat_variasi'] = ret['jumlah_pekerja'] * ret['tingkat_variasi_pekerjaan']\n",
    "    # ret['jumlah_pekerja_x_suku_bunga'] = ret['jumlah_pekerja'] * ret['suku_bunga_euribor_3bln']\n",
    "    # ret['jumlah_pekerja_x_ihk'] = ret['jumlah_pekerja'] * ret['indeks_harga_konsumen']\n",
    "    # ret['ratio_1'] = 1/(ret['jumlah_pekerja_x_tingkat_variasi'] + 1e6)\n",
    "    # ret['ratio_2'] = 1/(ret['jumlah_pekerja_x_suku_bunga'] + 1e6)\n",
    "    # ret['ratio_3'] = 1/(ret['jumlah_pekerja_x_ihk'] + 1e6)\n",
    "    ret['volatilitas'] = ret['tingkat_variasi_pekerjaan'] * ret['indeks_harga_konsumen']\n",
    "    ret['stability'] = 1 / (ret['volatilitas'] + 1e-6)\n",
    "    # ret['volatilitas_log'] = np.log1p(ret['volatilitas']+ 1e-6)\n",
    "    # ret['stability_log'] = np.log1p(ret['stability']+1e-6)\n",
    "    bins = [0, 25, 40, 60, 100]\n",
    "    labels = ['Gen-Z', 'Millenial', 'Gen-X', 'Boomer']\n",
    "    ret['kelompok_usia'] = pd.cut(ret['usia'], bins=bins, labels=labels)\n",
    "    # ret['feat1'] = (ret['gagal_bayar_sebelumnya'] == 'no') & (ret['jenis_kontak'] == 'cellular')\n",
    "    # ret['feat2'] = (ret['pinjaman_rumah'] == 'unknown') & (ret['pinjaman_pribadi'] == 'unknown')\n",
    "    # ret['feat3'] = (ret['pinjaman_rumah'] == 'no') & (ret['pinjaman_pribadi'] == 'no')\n",
    "    # ret['feat4'] = (ret['pekerjaan'] == 'pensiunan') & (ret['gagal_bayar_sebelumnya'] == 'no')\n",
    "    # ret['feat5'] = (ret['pekerjaan'] == 'pensiunan') & (ret['jenis_kontak'] == 'cellular')\n",
    "    ret['feat6'] = (ret['pekerjaan'] == 'pensiunan') & \\\n",
    "                    (ret['gagal_bayar_sebelumnya'] == 'no') & \\\n",
    "                    (ret['jenis_kontak'] == 'cellular')\n",
    "    \n",
    "    ret['interaksi_kontak_terakhir'] = ret['jumlah_kontak_kampanye_ini'] / (ret['hari_kontak_terakhir'] + 1) \n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "1e808481",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder4 = TargetEncoder()\n",
    "def apply_indeks(df: pd.DataFrame, train: bool = True) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    # ret['indeks_harga_konsumen'] = ret['indeks_harga_konsumen'].astype(str)\n",
    "    # if train:\n",
    "    #     ret['indeks_1'] = encoder3.fit_transform(ret['indeks_harga_konsumen'], ret['jumlah_pekerja'])\n",
    "    # else:\n",
    "    #     ret['indeks_1'] = encoder3.transform(ret['indeks_harga_konsumen'])\n",
    "    return ret\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "64243d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(\n",
    "    categories='auto',\n",
    "    drop='first',  \n",
    "    sparse_output=False,  \n",
    "    handle_unknown='ignore'  \n",
    ")\n",
    "def apply_sukses(df, is_train):\n",
    "    ret = df.copy()\n",
    "    if is_train:\n",
    "        encoded_fuel = ohe.fit_transform(ret[['hasil_kampanye_sebelumnya']])\n",
    "    else:\n",
    "        encoded_fuel = ohe.transform(ret[['hasil_kampanye_sebelumnya']])\n",
    "    \n",
    "    features = ohe.get_feature_names_out(['hasil_kampanye_sebelumnya'])\n",
    "    encoded_df = pd.DataFrame(encoded_fuel, columns=features, index=ret.index)\n",
    "    \n",
    "    ret = pd.concat([ret, encoded_df], axis=1)\n",
    "    ret = ret.drop(columns=['hasil_kampanye_sebelumnya'])\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "a3e9814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components=1, kernel='rbf', gamma=1)  \n",
    "\n",
    "def apply_kpca(df: pd.DataFrame, train: bool = False) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    cols = ['tingkat_variasi_pekerjaan', 'suku_bunga_euribor_3bln', 'jumlah_pekerja']\n",
    "    if train:\n",
    "        kpca_result = kpca.fit_transform(ret[cols])\n",
    "        kpca_cols = ['kpca_1']\n",
    "        ret = ret.drop(columns=cols)\n",
    "        ret[kpca_cols] = kpca_result\n",
    "    else:\n",
    "        kpca_result = kpca.transform(ret[cols])\n",
    "        kpca_cols = ['kpca_1']\n",
    "        ret = ret.drop(columns=cols)\n",
    "        ret[kpca_cols] = kpca_result\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "a10f6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "def apply_pca(df: pd.DataFrame, train: bool = False) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    cols = ['tingkat_variasi_pekerjaan', 'suku_bunga_euribor_3bln', 'jumlah_pekerja']\n",
    "    if train:\n",
    "        pca_result = pca.fit_transform(ret[cols])\n",
    "        pca_cols = ['pca_1']\n",
    "\n",
    "        ret = ret.drop(columns=cols)\n",
    "        ret[pca_cols] = pca_result\n",
    "    else:\n",
    "        pca_result = pca.transform(ret[cols])\n",
    "        pca_cols = ['pca_1']\n",
    "        ret = ret.drop(columns=cols)\n",
    "        ret[pca_cols] = pca_result\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "96d141b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_high_season(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ret = df.copy()\n",
    "    ret['high_season'] = ret['bulan_kontak_terakhir'].apply(lambda x: 1 if x in ['oct', 'mar', 'sep', 'apr', 'dec'] else 0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "44dd2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPipeline:\n",
    "    \n",
    "    def fit_transform(X: pd.DataFrame, y: pd.Series) -> tuple:\n",
    "        ret = pd.concat([X, y], axis=1)\n",
    "        ret = apply_label_encoding(ret, True)\n",
    "        ret = apply_sukses(ret, True)\n",
    "        ret = apply_fe(ret)\n",
    "        ret = apply_count_encoding(ret, True)\n",
    "        ret = apply_high_season(ret)\n",
    "        ret = apply_indeks(ret, True)\n",
    "        ret.drop(columns=['customer_number', 'usia', 'status_perkawinan',\n",
    "       'pendidikan', 'gagal_bayar_sebelumnya', 'pinjaman_rumah',\n",
    "       'pinjaman_pribadi', 'bulan_kontak_terakhir', 'jumlah_kontak_kampanye_ini',\n",
    "       'hari_sejak_kontak_sebelumnya', 'jumlah_kontak_sebelumnya',\n",
    "       'indeks_harga_konsumen', 'pulau', 'indeks_kepercayaan_konsumen'], inplace=True)\n",
    "        ret = apply_scale(ret, True)\n",
    "        ret = apply_pca(ret, True)\n",
    "        return (ret.drop(columns=[\"berlangganan_deposito\"]), ret[\"berlangganan_deposito\"])\n",
    "\n",
    "    def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        ret = df.copy()\n",
    "        ret = apply_label_encoding(ret, False)\n",
    "        ret = apply_sukses(ret, False)\n",
    "        ret = apply_fe(ret)\n",
    "        ret = apply_count_encoding(ret, False)\n",
    "        ret = apply_high_season(ret)\n",
    "        ret = apply_indeks(ret, False)\n",
    "        ret.drop(columns=['customer_number', 'usia', 'status_perkawinan',\n",
    "       'pendidikan', 'gagal_bayar_sebelumnya', 'pinjaman_rumah',\n",
    "         'pinjaman_pribadi', 'bulan_kontak_terakhir','jumlah_kontak_kampanye_ini',\n",
    "         'hari_sejak_kontak_sebelumnya', 'jumlah_kontak_sebelumnya',\n",
    "         'indeks_harga_konsumen', 'pulau', 'indeks_kepercayaan_konsumen'], inplace=True)\n",
    "        ret = apply_scale(ret, False)\n",
    "        ret = apply_pca(ret, False)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "999807df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2091, number of negative: 16241\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 295\n",
      "[LightGBM] [Info] Number of data points in the train set: 18332, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114063 -> initscore=-2.049896\n",
      "[LightGBM] [Info] Start training from score -2.049896\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10827\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 287\n",
      "[LightGBM] [Info] Number of data points in the train set: 12221, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114066 -> initscore=-2.049866\n",
      "[LightGBM] [Info] Start training from score -2.049866\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10827\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 283\n",
      "[LightGBM] [Info] Number of data points in the train set: 12221, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114066 -> initscore=-2.049866\n",
      "[LightGBM] [Info] Start training from score -2.049866\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "Fold 1 ROC AUC: 0.7835\n",
      "[LightGBM] [Info] Number of positive: 2091, number of negative: 16242\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 295\n",
      "[LightGBM] [Info] Number of data points in the train set: 18333, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 282\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 287\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "Fold 2 ROC AUC: 0.8071\n",
      "[LightGBM] [Info] Number of positive: 2091, number of negative: 16242\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 295\n",
      "[LightGBM] [Info] Number of data points in the train set: 18333, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 281\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 283\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 283\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "Fold 3 ROC AUC: 0.8070\n",
      "[LightGBM] [Info] Number of positive: 2091, number of negative: 16242\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 297\n",
      "[LightGBM] [Info] Number of data points in the train set: 18333, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 284\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 288\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "Fold 4 ROC AUC: 0.7945\n",
      "[LightGBM] [Info] Number of positive: 2092, number of negative: 16241\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 296\n",
      "[LightGBM] [Info] Number of data points in the train set: 18333, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114111 -> initscore=-2.049418\n",
      "[LightGBM] [Info] Start training from score -2.049418\n",
      "[LightGBM] [Info] Number of positive: 1395, number of negative: 10827\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 285\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114138 -> initscore=-2.049149\n",
      "[LightGBM] [Info] Start training from score -2.049149\n",
      "[LightGBM] [Info] Number of positive: 1395, number of negative: 10827\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 286\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114138 -> initscore=-2.049149\n",
      "[LightGBM] [Info] Start training from score -2.049149\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 287\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n",
      "Fold 5 ROC AUC: 0.7983\n",
      "\n",
      "=== ROC AUC Scores Across Folds ===\n",
      "['0.7835', '0.8071', '0.8070', '0.7945', '0.7983']\n",
      "Average ROC AUC: 0.7981 ± 0.0088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def run_kfold_cv(model, df, n_splits=5, random_state=42):\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    metrics = {\n",
    "        'roc_auc': [],\n",
    "    }\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df, df['berlangganan_deposito']), 1):\n",
    "        train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "        y_train, y_val = train_df['berlangganan_deposito'], val_df['berlangganan_deposito']\n",
    "        \n",
    "        \n",
    "        X_train = train_df.drop(columns=['berlangganan_deposito'])\n",
    "        X_val = val_df.drop(columns=['berlangganan_deposito'])\n",
    "\n",
    "        X_train_transformed, y_train = CustomPipeline.fit_transform(X_train, y_train)\n",
    "        X_val_transformed = CustomPipeline.transform(X_val)\n",
    "\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "\n",
    "        y_val_pred = model.predict_proba(X_val_transformed)[:, 1]\n",
    "\n",
    "        roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "        metrics['roc_auc'].append(roc_auc)\n",
    "\n",
    "        print(f\"Fold {fold} ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    avg_roc_auc = np.mean(metrics['roc_auc'])\n",
    "    std_roc_auc = np.std(metrics['roc_auc'])\n",
    "    \n",
    "\n",
    "    print(\"\\n=== ROC AUC Scores Across Folds ===\")\n",
    "    print([f\"{score:.4f}\" for score in metrics['roc_auc']])\n",
    "    print(f\"Average ROC AUC: {avg_roc_auc:.4f} ± {std_roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# model = RandomForestClassifier(random_state=42)\n",
    "# model = GradientBoostingClassifier( n_estimators=300,\n",
    "#     learning_rate=0.05,\n",
    "#     max_depth=4,\n",
    "#     subsample=0.8,\n",
    "#     min_samples_leaf=5,\n",
    "#     max_features='sqrt',\n",
    "#     validation_fraction=0.1,\n",
    "#     n_iter_no_change=10,\n",
    "#     random_state=42)\n",
    "# model = LGBMClassifier()\n",
    "# model = XGBClassifier()\n",
    "# model = CatBoostClassifier()\n",
    "# model = AdaBoostClassifier()\n",
    "# model = LogisticRegression()\n",
    "# model = DecisionTreeClassifier()\n",
    "# model = KNeighborsClassifier()\n",
    "# model = StackingClassifier(estimators=[\n",
    "#     ('rf', RandomForestClassifier(random_state=42, class_weight='balanced')),\n",
    "#     ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "#     ('xgb', XGBClassifier()),\n",
    "#     ('cat', CatBoostClassifier()),\n",
    "#     ('lgbm', LGBMClassifier()),\n",
    "#     ('ada', AdaBoostClassifier()),\n",
    "#     ('logreg', LogisticRegression()),\n",
    "#     ('dt', DecisionTreeClassifier())\n",
    "# ], final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\n",
    "\n",
    "# model = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         ('lgbm', LGBMClassifier()),\n",
    "#         ('xgb', XGBClassifier()),\n",
    "#         ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42))\n",
    "#     ],\n",
    "#     voting='soft'  # Place this INSIDE the parentheses\n",
    "# )\n",
    "\n",
    "\n",
    "model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm', LGBMClassifier()),\n",
    "        ('xgb', XGBClassifier()),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "        \n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=3\n",
    ")\n",
    "# model = SVC(probability=True, random_state=42)\n",
    "# model = BalancedRandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "run_kfold_cv(model, train_df, n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdcc7a",
   "metadata": {},
   "source": [
    "=== ROC AUC Scores Across Folds ===\n",
    "['0.7835', '0.8071', '0.8070', '0.7945', '0.7983']\n",
    "Average ROC AUC: 0.7981 ± 0.0088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "57152279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2091, number of negative: 16241\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 295\n",
      "[LightGBM] [Info] Number of data points in the train set: 18332, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114063 -> initscore=-2.049896\n",
      "[LightGBM] [Info] Start training from score -2.049896\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10827\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 284\n",
      "[LightGBM] [Info] Number of data points in the train set: 12221, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114066 -> initscore=-2.049866\n",
      "[LightGBM] [Info] Start training from score -2.049866\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10827\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 283\n",
      "[LightGBM] [Info] Number of data points in the train set: 12221, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114066 -> initscore=-2.049866\n",
      "[LightGBM] [Info] Start training from score -2.049866\n",
      "[LightGBM] [Info] Number of positive: 1394, number of negative: 10828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 282\n",
      "[LightGBM] [Info] Number of data points in the train set: 12222, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114057 -> initscore=-2.049958\n",
      "[LightGBM] [Info] Start training from score -2.049958\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: StackingClassifier(cv=3,\n                   estimators=[('lgbm', LGBMClassifier()),\n                               ('xgb',\n                                XGBClassifier(base_score=None, booster=None,\n                                              callbacks=None,\n                                              colsample_bylevel=None,\n                                              colsample_bynode=None,\n                                              colsample_bytree=None,\n                                              device=None,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None,\n                                              feature_types=None, gamma=None,\n                                              grow_policy=None,\n                                              importance_type=None,\n                                              interac...\n                                              learning_rate=None, max_bin=None,\n                                              max_cat_threshold=None,\n                                              max_cat_to_onehot=None,\n                                              max_delta_step=None,\n                                              max_depth=None, max_leaves=None,\n                                              min_child_weight=None,\n                                              missing=nan,\n                                              monotone_constraints=None,\n                                              multi_strategy=None,\n                                              n_estimators=None, n_jobs=None,\n                                              num_parallel_tree=None,\n                                              random_state=None, ...)),\n                               ('gb',\n                                GradientBoostingClassifier(random_state=42))],\n                   final_estimator=LogisticRegression())",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[702]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m X_test = CustomPipeline.transform(X_test)\n\u001b[32m      9\u001b[39m model.fit(X_train, y_train)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m explainer = \u001b[43mshap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m shap_values = explainer(X_train, check_additivity=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     13\u001b[39m shap.summary_plot(shap_values, X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\shap\\explainers\\_explainer.py:206\u001b[39m, in \u001b[36mExplainer.__init__\u001b[39m\u001b[34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m             algorithm = \u001b[33m\"\u001b[39m\u001b[33mpermutation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001b[39;00m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    207\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m             + \u001b[38;5;28mstr\u001b[39m(model)\n\u001b[32m    209\u001b[39m         )\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# build the right subclass\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m algorithm == \u001b[33m\"\u001b[39m\u001b[33mexact\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: StackingClassifier(cv=3,\n                   estimators=[('lgbm', LGBMClassifier()),\n                               ('xgb',\n                                XGBClassifier(base_score=None, booster=None,\n                                              callbacks=None,\n                                              colsample_bylevel=None,\n                                              colsample_bynode=None,\n                                              colsample_bytree=None,\n                                              device=None,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None,\n                                              feature_types=None, gamma=None,\n                                              grow_policy=None,\n                                              importance_type=None,\n                                              interac...\n                                              learning_rate=None, max_bin=None,\n                                              max_cat_threshold=None,\n                                              max_cat_to_onehot=None,\n                                              max_delta_step=None,\n                                              max_depth=None, max_leaves=None,\n                                              min_child_weight=None,\n                                              missing=nan,\n                                              monotone_constraints=None,\n                                              multi_strategy=None,\n                                              n_estimators=None, n_jobs=None,\n                                              num_parallel_tree=None,\n                                              random_state=None, ...)),\n                               ('gb',\n                                GradientBoostingClassifier(random_state=42))],\n                   final_estimator=LogisticRegression())"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train = train_set.drop(columns=[\"berlangganan_deposito\"])\n",
    "y_train = train_set[\"berlangganan_deposito\"]\n",
    "X_test = val_set.drop(columns=[\"berlangganan_deposito\"])\n",
    "y_test = val_set[\"berlangganan_deposito\"]\n",
    "# X_train, y_train = CustomPipeline.fit_transform(X_train, y_train)\n",
    "(X_train, y_train) = CustomPipeline.fit_transform(X_train, y_train)\n",
    "X_test = CustomPipeline.transform(X_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_train, check_additivity=False)\n",
    "\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "shap.plots.force(shap_values[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
